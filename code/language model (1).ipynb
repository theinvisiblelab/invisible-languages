{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd502d8-6344-45ec-9c43-82f129d4a3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI/bge-reranker-v2-m3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alibaba-NLP/gte-multilingual-reranker-base</td>\n",
       "      <td>af, ar, az, be, bg, bn, ca, cs, cy, da, de, el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProsusAI/finbert</td>\n",
       "      <td>en, tf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAAI/bge-reranker-large</td>\n",
       "      <td>en, zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook/bart-large-mnli</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>DeepChem/SmilesTokenizer_PubChem_1M</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>KBLab/sentence-bert-swedish-cased</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>T-Systems-onsite/cross-en-de-roberta-sentence-...</td>\n",
       "      <td>de, en, tf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>shaktiman404/dummy-bert-base-cased</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>emrecan/bert-base-turkish-cased-mean-nli-stsb-tr</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              model_id  \\\n",
       "0                              BAAI/bge-reranker-v2-m3   \n",
       "1           Alibaba-NLP/gte-multilingual-reranker-base   \n",
       "2                                     ProsusAI/finbert   \n",
       "3                              BAAI/bge-reranker-large   \n",
       "4                             facebook/bart-large-mnli   \n",
       "..                                                 ...   \n",
       "645                DeepChem/SmilesTokenizer_PubChem_1M   \n",
       "646                  KBLab/sentence-bert-swedish-cased   \n",
       "647  T-Systems-onsite/cross-en-de-roberta-sentence-...   \n",
       "648                 shaktiman404/dummy-bert-base-cased   \n",
       "649   emrecan/bert-base-turkish-cased-mean-nli-stsb-tr   \n",
       "\n",
       "                                             languages  \n",
       "0                                                       \n",
       "1    af, ar, az, be, bg, bn, ca, cs, cy, da, de, el...  \n",
       "2                                               en, tf  \n",
       "3                                               en, zh  \n",
       "4                                                       \n",
       "..                                                 ...  \n",
       "645                                                     \n",
       "646                                                 sv  \n",
       "647                                         de, en, tf  \n",
       "648                                                     \n",
       "649                                                 tr  \n",
       "\n",
       "[650 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All detected languages in NLP-related models: ['T5', 'aa', 'ab', 'ae', 'af', 'ak', 'am', 'an', 'ar', 'as', 'av', 'ay', 'az', 'ba', 'be', 'bg', 'bi', 'bm', 'bn', 'bo', 'br', 'bs', 'ca', 'ce', 'ch', 'co', 'cs', 'cv', 'cy', 'da', 'de', 'dv', 'dz', 'ee', 'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'ff', 'fi', 'fj', 'fo', 'fr', 'fy', 'ga', 'gd', 'ge', 'gl', 'gn', 'gu', 'gv', 'ha', 'he', 'hi', 'ho', 'hr', 'ht', 'hu', 'hy', 'id', 'ig', 'io', 'is', 'it', 'iu', 'iw', 'ja', 'jv', 'ka', 'kg', 'ki', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'ks', 'ku', 'kv', 'kw', 'ky', 'la', 'lb', 'lg', 'li', 'lm', 'ln', 'lo', 'lt', 'lu', 'lv', 'mg', 'mh', 'mi', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'my', 'na', 'nb', 'ne', 'nl', 'nn', 'no', 'nr', 'ns', 'nv', 'ny', 'oc', 'oj', 'om', 'or', 'os', 'pa', 'pl', 'ps', 'pt', 'qa', 'qu', 'r1', 'rm', 'rn', 'ro', 'rp', 'ru', 'rw', 'sa', 'sc', 'sd', 'se', 'sg', 'si', 'sk', 'sl', 'sm', 'sn', 'so', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 't5', 'ta', 'te', 'tf', 'tg', 'th', 'ti', 'tk', 'tl', 'tn', 'to', 'tr', 'ts', 'tt', 'tw', 'ty', 'ud', 'ug', 'uk', 'ur', 'uz', 've', 'vi', 'vn', 'vo', 'wa', 'wo', 'xh', 'yi', 'yo', 'za', 'zh', 'zu']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "\n",
    "# Ignore irrelevant warnings from Hugging Face API\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize API\n",
    "api = HfApi()\n",
    "\n",
    "# Define NLP-related tasks\n",
    "nlp_tasks = [\n",
    "    \"text-classification\",  \n",
    "    \"token-classification\",  \n",
    "    \"question-answering\",  \n",
    "    \"translation\",  \n",
    "    \"summarization\",  \n",
    "    \"text-generation\",  \n",
    "    \"fill-mask\",  \n",
    "    \"sentence-similarity\",  \n",
    "    \"feature-extraction\",  \n",
    "    \"table-question-answering\",  \n",
    "    \"zero-shot-classification\",  \n",
    "    \"text2text-generation\",  \n",
    "    \"text-embeddings-inference\"  \n",
    "]\n",
    "\n",
    "model_data = []\n",
    "all_languages = set()\n",
    "\n",
    "for task in nlp_tasks:\n",
    "    try:\n",
    "        models = api.list_models(filter=task, limit=50)  # Retrieve up to 50 models per task\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve models for task {task}, error: {e}\")\n",
    "        continue  # Skip this task and proceed to the next one\n",
    "\n",
    "    for model in models:\n",
    "        model_id = model.id\n",
    "        try:\n",
    "            # Get detailed model information\n",
    "            model_info = api.model_info(model_id)\n",
    "            tags = model_info.tags if model_info.tags else []\n",
    "\n",
    "            # Extract possible language tags\n",
    "            languages = sorted({tag for tag in tags if len(tag) == 2 or tag.startswith(\"language:\")})\n",
    "\n",
    "            all_languages.update(languages)  # Record all detected languages\n",
    "\n",
    "            model_data.append({\n",
    "                \"model_id\": model_id,\n",
    "                \"languages\": \", \".join(languages)  # Store languages as a string for readability\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve details for {model_id}, skipping this model: {e}\")\n",
    "            continue  # Skip this model and proceed to the next one\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(model_data)\n",
    "\n",
    "# Display final NLP-related model data\n",
    "display(df)\n",
    "\n",
    "# Print all detected language tags\n",
    "print(\"All detected languages in NLP-related models:\", sorted(all_languages))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
